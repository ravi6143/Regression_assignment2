{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04f74af-8e6e-4fc9-9bce-edc63f95af4b",
   "metadata": {},
   "source": [
    "## Question - 1\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1bc355-c6dc-4360-a8f1-61fd53d751d5",
   "metadata": {},
   "source": [
    "\n",
    "R-squared (coefficient of determination) is a statistical measure used in regression analysis to assess the proportion of variation in the dependent variable that is explained by the independent variables in a regression model.\n",
    "\n",
    "Here's a breakdown of its concept, calculation, and interpretation:\n",
    "\n",
    "Concept:\n",
    "* .Definition: R-squared quantifies the goodness of fit of a regression model. It ranges from 0 to 1, where 0 indicates that the model does not explain any variability in the dependent variable, and 1 implies that the model perfectly explains all the variability.\n",
    "\n",
    "* .Interpretation: It provides an indication of how well the independent variables (predictors) explain the variability of the dependent variable (response) in the regression model.\n",
    "\n",
    ">> Calculation:\n",
    "Formula: R-squared is calculated as the proportion of the total variation in the dependent variable \n",
    "\n",
    "Y that is explained by the variation in the independent variables (predicted by the model) compared to the total variation in Y.\n",
    "\n",
    "R^2 = 1− SS_residual / SS_total\n",
    "\n",
    "SS_residual\n",
    "(Sum of Squares Residual): Measures the variation that is not explained by the regression model.\n",
    "\n",
    "SS_total\n",
    "(Total Sum of Squares): Measures the total variation in the dependent variable.\n",
    "\n",
    "\n",
    "* . Interpretation:\n",
    "\n",
    "-. Higher R-squared: A higher value of R-squared (closer to 1) indicates that a larger proportion of the variability in the dependent variable is explained by the independent variables in the model. It implies that the model fits the data well.\n",
    "\n",
    "-. Lower R-squared: A lower value of R-squared (closer to 0) suggests that the model does not explain much of the variability in the dependent variable, indicating that it might not be the best fit for the data.\n",
    "\n",
    "* .Limitations:\n",
    "\n",
    "1. R-squared can be misleading when used inappropriately or without considering other factors. A high R-squared doesn’t necessarily imply that the model's predictions are accurate or that the model is good.\n",
    "\n",
    "2. Adding more independent variables to the model tends to increase R-squared, even if those variables are not truly helpful in predicting the dependent variable. Therefore, adjusted R-squared (which accounts for the number of predictors) might be more useful in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e832e29-d017-48c6-b8bf-ccf65da57b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf3890-049e-4ab3-a438-3073fb40b6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "420f7822-9297-4d49-b7a5-34407db72462",
   "metadata": {},
   "source": [
    "## Question - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001c941-36a1-4e09-86f6-c4164c4501d9",
   "metadata": {},
   "source": [
    "\n",
    "Adjusted R-squared is a modified version of the regular R-squared that considers the number of predictors (independent variables) in a regression model. It addresses some limitations of the regular R-squared when evaluating the goodness of fit of a model.\n",
    "\n",
    "--. Differences between Regular R-squared and Adjusted R-squared:\n",
    "\n",
    "* .Definition:\n",
    "\n",
    "1. Regular R-squared (R²) measures the proportion of variability in the dependent variable explained by the independent variables in the model.\n",
    "\n",
    "2. Adjusted R-squared (Adjusted R²) also assesses the proportion of variation in the dependent variable explained by the independent variables but adjusts for the number of predictors in the model.\n",
    "\n",
    "\n",
    "Calculation:\n",
    "\n",
    "Regular R-squared is calculated using the formula:\n",
    "R^2 = 1− SS_total / SS_residual\n",
    "\n",
    "\n",
    "Adjusted R-squared is calculated as:\n",
    "\n",
    "Adjusted R^2 = 1- (1-R^2)(n-1) / n-k-1\n",
    " \n",
    "where:\n",
    "n = number of observations\n",
    "\n",
    "k = number of predictors (independent variables)\n",
    "\n",
    "# .Purpose:\n",
    "\n",
    "Regular R-squared evaluates the goodness of fit but does not consider the complexity added by additional predictors.\n",
    "\n",
    "Adjusted R-squared penalizes the addition of unnecessary predictors. It adjusts the R-squared value based on the number of predictors, preventing an artificially inflated R-squared when including more variables.\n",
    "\n",
    "# .Interpretation:\n",
    "\n",
    "Regular R-squared tends to increase (or stay the same) when adding more predictors, even if they are not meaningful, which can lead to overfitting.\n",
    "\n",
    "Adjusted R-squared generally decreases if adding irrelevant predictors, providing a more accurate reflection of the model's goodness of fit relative to the number of predictors.\n",
    "\n",
    "\n",
    "* .Comparative Analysis:\n",
    "\n",
    "Regular R-squared might favor complex models with many predictors, potentially leading to a misleading assessment of model performance.\n",
    "\n",
    "Adjusted R-squared is preferred when comparing models with different numbers of predictors since it penalizes overly complex models and helps select the model that strikes the best balance between simplicity and explanatory power.\n",
    "\n",
    "\n",
    ">> In summary, adjusted R-squared is a more reliable measure when assessing the goodness of fit of a regression model by considering the trade-off between model complexity (number of predictors) and the proportion of variability explained in the dependent variable. It helps in better model selection by penalizing unnecessary predictors and is useful for comparing models with different numbers of predictors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce3542-1f49-417e-b93a-b79ef060e931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd9c6e-f6ad-4efa-82c2-dd80bd422d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba6cb9-7514-4573-baf9-92a0982aa82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4946e035-718e-4d76-a181-57c8407fd4d4",
   "metadata": {},
   "source": [
    "## Question - 3\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7771b-6ca2-4658-a0ad-2f9040c59b14",
   "metadata": {},
   "source": [
    "\n",
    "Adjusted R-squared is more appropriate and recommended in the following scenarios:\n",
    "\n",
    "1. Comparing Models with Different Numbers of Predictors:\n",
    "\n",
    "When comparing multiple regression models with different numbers of predictors, adjusted R-squared helps in model selection by penalizing the inclusion of unnecessary variables. It accounts for the model's complexity, aiding in choosing a more parsimonious model.\n",
    "\n",
    "\n",
    "2. Avoiding Overfitting:\n",
    "\n",
    "To prevent overfitting, especially when dealing with a large number of predictors relative to the sample size, adjusted R-squared provides a more conservative measure of model goodness of fit. It adjusts the R-squared value for the sample size and the number of predictors, discouraging the inclusion of too many predictors that may not contribute significantly to the model's explanatory power.\n",
    "\n",
    "\n",
    "3. Complex Models:\n",
    "\n",
    "In situations where complex models are created with numerous predictors, regular R-squared might be misleadingly high due to the inclusion of irrelevant variables. Adjusted R-squared provides a more reliable assessment of the model's goodness of fit by considering the model's complexity.\n",
    "\n",
    "\n",
    "4. Model Comparison for Parsimony:\n",
    "\n",
    "When deciding between competing models, adjusted R-squared aids in choosing the most suitable model by balancing simplicity with explanatory power. It favors models that achieve higher explanatory power while utilizing fewer predictors.\n",
    "\n",
    "5. Avoiding Misleading Interpretations:\n",
    "\n",
    "If regular R-squared is high but the number of predictors is also high, it might lead to a misleading interpretation of model effectiveness. Adjusted R-squared offers a more realistic evaluation of how well the model fits the data, considering the trade-off between the number of predictors and goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c9daf-3cf7-4ccd-9b88-69c4df4f1b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cde98-a077-423b-b0f5-acbc0dc15d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794be6c-9121-4d80-8227-5b879c48f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0917296d-54d7-4494-89cc-2e8906c80381",
   "metadata": {},
   "source": [
    "## Question - 4\n",
    "ans- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f299def7-c397-4253-b552-322196f4fb24",
   "metadata": {},
   "source": [
    "In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the performance of regression models by measuring the differences between predicted values and actual observed values.\n",
    "\n",
    "1. Mean Squared Error (MSE):\n",
    "\n",
    "MSE calculates the average of the squared differences between predicted and actual values.\n",
    "Mathematically, MSE is calculated by taking the mean of the squared residuals (differences) between predicted and actual values for all data points.\n",
    "It's calculated as: \n",
    "\n",
    "MSE= 1/n ∑ n,i=1 (yi− yi^)^2\n",
    " , where \n",
    "yi is the observed value, \n",
    "\n",
    "yi^ is the predicted value, and \n",
    "\n",
    "n is the number of data points.\n",
    "\n",
    "MSE penalizes larger errors more due to squaring.\n",
    "\n",
    "\n",
    "2. Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE is the square root of the MSE. It represents the standard deviation of the residuals, which gives an idea of the average deviation of predicted values from actual values.\n",
    "Mathematically, RMSE is calculated as: \n",
    "RMSE = sqrt(MSE)\n",
    "\n",
    "It's in the same units as the dependent variable, making it more interpretable.\n",
    "\n",
    "3. Mean Absolute Error (MAE):\n",
    "\n",
    "MAE calculates the average of the absolute differences between predicted and actual values.\n",
    "Mathematically, MAE is calculated by taking the mean of the absolute residuals between predicted and actual values for all data points.\n",
    "It's calculated as: \n",
    "\n",
    "MAE= 1/n ∑ n,i=1 ∣yi − yi^∣.\n",
    "\n",
    "MAE does not penalize larger errors as heavily as MSE because it doesn't involve squaring the errors.\n",
    "These metrics are used to assess how well a regression model predicts the outcome variable. Lower values of RMSE, MSE, and MAE indicate better predictive performance, with RMSE and MSE particularly sensitive to larger errors due to the squaring, while MAE gives an average of absolute differences. Choosing the most appropriate metric depends on the specific context and requirements of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c19ecd-5df9-485e-8115-9914b69f259b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539a4ae-0d0f-41c7-a9ca-4fc490204d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3d8c3-08b6-4acf-9ea1-a9acffad8552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc163171-f962-4b34-a8b1-ab0704a59292",
   "metadata": {},
   "source": [
    "## Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecff3b6-be74-44f1-98ed-7ef1a9abee1a",
   "metadata": {},
   "source": [
    ">>Advantages:\n",
    "\n",
    "1. RMSE (Root Mean Squared Error):\n",
    "\n",
    "* .Sensitive to large errors: RMSE penalizes large errors more heavily due to the squaring operation, making it more effective in capturing the impact of outliers.\n",
    "\n",
    "* .Popular metric: It is widely used and easy to interpret since it represents the standard deviation of the residuals.\n",
    "\n",
    "\n",
    "2. MSE (Mean Squared Error):\n",
    "\n",
    "* .Differentiability: Mathematically differentiable and convex, making it suitable for optimization algorithms.\n",
    "\n",
    "* .Greater penalty for larger errors: Like RMSE, it heavily penalizes larger errors due to the squaring operation, which can be advantageous in certain scenarios.\n",
    "\n",
    "3. MAE (Mean Absolute Error):\n",
    "\n",
    "* .Robust to outliers: MAE is not as sensitive to outliers as RMSE and MSE since it doesn't involve squaring the errors.\n",
    "\n",
    "* .Simplicity: Easier to interpret and understand since it represents the average magnitude of errors.\n",
    "\n",
    "\n",
    ">> Disadvantages:\n",
    "\n",
    "1. RMSE (Root Mean Squared Error):\n",
    "\n",
    "* .Sensitivity to outliers: RMSE is highly sensitive to outliers due to the squaring of errors, which might not be desirable in certain situations.\n",
    "\n",
    "* .Bias towards larger errors: While beneficial for some analyses, the emphasis on larger errors might not align with the priority of minimizing all errors equally in certain applications.\n",
    "\n",
    "2. MSE (Mean Squared Error):\n",
    "\n",
    "* .Same issues with sensitivity to outliers: Similar to RMSE, MSE's emphasis on larger errors can be a drawback in scenarios where outliers need less focus.\n",
    "\n",
    "3. MAE (Mean Absolute Error):\n",
    "\n",
    "* .Less sensitivity to large errors: While being robust to outliers is an advantage, it might not effectively capture the impact of large errors, leading to less emphasis on those errors during model training.\n",
    "\n",
    ">>Choosing the Right Metric:\n",
    "The choice of metric depends on the specific problem, objectives, and data characteristics.\n",
    "RMSE and MSE are useful when large errors need more penalty and when the focus is on predicting accurately.\n",
    "MAE might be preferable when outliers need to be downplayed, and a more balanced error assessment is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69d009-360a-4991-95c2-b3016f55fb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cba70-fd19-4f67-b62d-f8c59f95fe48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8385255-0704-40df-8bd3-ff338b9667f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cf50c6a-414a-4a4e-8000-e01edc9f4d9f",
   "metadata": {},
   "source": [
    "## Question - 6\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0044c-83ae-4a3d-b3be-7e20493ddd0c",
   "metadata": {},
   "source": [
    "Lasso :- \n",
    "\n",
    "Lasso is regression analysis method that performs boht variable selection and regularization in oreder to enhance the prediction accuracy and interpretability of the resulting model.\n",
    "\n",
    "* . Difference\n",
    "\n",
    "The difference between Lasso and Ridge is that Lasso used for feature selection while Ridge is used to reducing the overfitting of a regression model.\n",
    "Ridge dont zero the error but penalize them to small while Lasso penalize the coefficients to zero which are less impactable.\n",
    "\n",
    "\n",
    "Use Case:\n",
    "\n",
    "-. Lasso is considered when dealing with high dimensional datasets with many features where it is needed to select the feature.\n",
    "It particularly used where the need to identify the most relevent predictors.\n",
    "\n",
    "-. Use Ridge when multicollinearity (high correlation among predictors) is present and when a simpler model that doesn't eliminate any predictors entirely is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a251cc-c7ed-4dff-bfbe-4e9adae612ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd03121-f8be-483c-aaa4-1c769989ba3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4f6b712-4645-41c7-92a3-1c6694672012",
   "metadata": {},
   "source": [
    "## Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273962a-765c-4964-846a-97ffed2aafab",
   "metadata": {},
   "source": [
    "Regularized linear models are employed to prevent overfitting by introducing a penalty term to the standard linear regression model, controlling the complexity of the model.\n",
    "\n",
    "One common type of regularization is Ridge Regression, which adds a penalty term proportional to the square of the magnitude of coefficients to the linear regression cost function. This regularization term penalizes large coefficients, which helps in preventing the model from fitting the noise in the data too closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e9f7b3-9035-4783-90fd-c7ee341b710c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.07607611587021253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de4f79-f81b-419c-81bb-e01e69f511e2",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "make_regression() generates synthetic regression data.\n",
    "\n",
    "Ridge() is employed to fit a Ridge Regression model to the training data.\n",
    "\n",
    "alpha is a hyperparameter controlling the strength of the regularization term. Higher alpha values impose stronger regularization.\n",
    "\n",
    "The model is evaluated using the mean squared error (MSE) metric on the test set.\n",
    "Regularized linear models like Ridge Regression help prevent overfitting by penalizing complex models (models with large coefficients) during training, thus favoring simpler models that generalize better to new, unseen data. Adjusting the regularization parameter allows controlling the trade-off between fitting the training data and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d410f-caa2-484b-ba18-5141f3fb9c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b610a88-7ce2-4ca4-bc52-fb2799bf3da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7d217b7-fdaa-4e74-afd8-d9c9bb9f9d5e",
   "metadata": {},
   "source": [
    "## Question - 8\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7fa6a-afb8-4d73-afe7-9eb4536c1c79",
   "metadata": {},
   "source": [
    "\n",
    "Regularized linear models, such as Ridge Regression, Lasso Regression, and ElasticNet, are powerful techniques for handling overfitting and improving the generalization of linear models. However, they do have some limitations that might make them less suitable or effective in certain scenarios:\n",
    "\n",
    "1. Black-Box Nature: Regularization techniques add penalty terms to the cost function to control the model's complexity. The resulting coefficients might not be as interpretable as those from standard linear regression, making it challenging to explain the relationship between predictors and the target variable.\n",
    "\n",
    "2. Impact of Hyperparameters: Regularization methods require tuning hyperparameters (like alpha in Ridge and Lasso) to achieve optimal performance. The choice of these hyperparameters can significantly impact model performance, and finding the right values often involves experimentation and cross-validation.\n",
    "\n",
    "3. Sensitivity to Outliers: Lasso Regression, which performs feature selection by driving some coefficients to zero, can be sensitive to outliers in the dataset. Outliers might disproportionately influence coefficient estimates, affecting model performance.\n",
    "\n",
    "4. Ineffective with Non-linear Relationships: Regularized linear models assume a linear relationship between predictors and the target variable. If the relationship is highly non-linear, these models might not capture complex patterns present in the data effectively.\n",
    "\n",
    "5. Collinearity Issues: Ridge Regression is effective in handling multicollinearity, but Lasso Regression tends to arbitrarily select one feature among highly correlated features and set others to zero. This behavior can be problematic when dealing with correlated predictors.\n",
    "\n",
    "6. Less Impact on Sparse Data: In scenarios where the dataset is sparse, meaning it contains a large number of features with many having minimal effect on the target variable, regularized linear models might not perform as well.\n",
    "\n",
    "7. Computationally Intensive: Depending on the size of the dataset and the number of features, the training of regularized linear models, particularly Lasso Regression with feature selection, can be computationally intensive.\n",
    "\n",
    "8. Preservation of Causal Relationships: While regularized models can aid in prediction tasks, they might not necessarily maintain causal relationships between predictors and the target variable, especially when feature selection is involved.\n",
    "\n",
    "Considering these limitations, it's essential to assess the nature of the problem, the characteristics of the data, and the desired interpretability of the model before deciding to employ regularized linear models in regression analysis. In some cases, other techniques such as tree-based models, nonlinear regression models, or more sophisticated approaches might be more appropriate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d69463-b5ce-4d8a-8803-115c9bc38184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ead89-e20b-420d-87a7-3bb5778581a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e47465-c2e7-41e9-b692-b76988dbf31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b033d07-5070-4e59-bab0-965fb4c27795",
   "metadata": {},
   "source": [
    "## Question - 9\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef7b4d-b213-486b-8748-d39aef5fdca0",
   "metadata": {},
   "source": [
    "Choosing between different evaluation metrics depends on the specific context of the problem and the priority given to certain characteristics of the models. In this scenario:\n",
    "\n",
    "* .Model A has an RMSE (Root Mean Squared Error) of 10.\n",
    "* .Model B has an MAE (Mean Absolute Error) of 8.\n",
    "\n",
    "Both RMSE and MAE are metrics used to evaluate regression models, measuring the average magnitude of the errors between predicted and actual values. Here's how they differ:\n",
    "\n",
    "RMSE gives higher weight to large errors since it squares the errors before taking the square root and then averaging them. It penalizes larger errors more severely.\n",
    "\n",
    "MAE calculates the average absolute errors without considering their direction, providing a more straightforward measure of average error.\n",
    "\n",
    ">>Choosing the better model depends on the context:\n",
    "\n",
    "If the problem demands higher penalties for larger errors or outliers, Model A (with RMSE of 10) might be preferred because RMSE penalizes larger errors more, thus emphasizing the importance of accuracy in larger deviations.\n",
    "\n",
    "On the other hand, if the focus is on the average error's magnitude without placing much emphasis on larger errors, Model B (with MAE of 8) might be preferable. MAE is less sensitive to outliers and might better represent the average prediction error.\n",
    "\n",
    "* .Limitations to consider:\n",
    "\n",
    "1. Sensitivity to Outliers: RMSE is more sensitive to outliers due to squaring the errors. A few large errors in the dataset could significantly impact the RMSE.\n",
    "\n",
    "2. Interpretability: MAE might be more interpretable as it directly represents the average absolute error without any complex transformations.\n",
    "\n",
    "Ultimately, the choice between RMSE and MAE depends on the problem's specific requirements, the significance of outliers, and whether emphasizing larger errors is crucial or not. Both metrics have their strengths and limitations, and the choice should be made considering the particular context and priorities of the problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41af38-cd1b-41a7-a630-2169f37b2ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6aafa-db7d-46a7-8dc2-f1a4d688bb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503fe731-725c-42c8-91bd-b8a054f8c791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1f738d-a9c1-4e69-b06e-412a4aae5c90",
   "metadata": {},
   "source": [
    "## Question - 10\n",
    "ans - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d7bf6-9b6c-4851-bef5-f643f9fe0884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
